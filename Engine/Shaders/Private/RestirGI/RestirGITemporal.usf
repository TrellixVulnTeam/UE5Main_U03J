#include "../Common.ush"
#include "../DeferredShadingCommon.ush"
#include "../SceneTextureParameters.ush"
#include "../ScreenSpaceDenoise/SSDPublic.ush"
#include "../MortonCode.ush"
#include "../BRDF.ush"

#include "../RayTracing/RayTracingCommon.ush"
#include "../RayTracing/RayTracingReflectionsCommon.ush"
#include "../RayTracing/RayTracingDeferredReflections.ush"
#include "../PathTracing/Utilities/PathTracingRandomSequence.ush" 

#include "ReservoirManagement.ush"
#include "RandomNumberUtils.ush"

Texture2D<float>    DepthHistory;
Texture2D<float4>   NormalHistory;

float4  HistoryScreenPositionScaleBias;
uint    MaxTemporalHistory;
uint    UpscaleFactor;
float   TemporalDepthRejectionThreshold;
float   TemporalNormalRejectionThreshold;
int     InputSlice;
int     OutputSlice;
int     HistoryReservoir;

float2 GetReprojectedBufferUV(float2 ScreenPos, float SceneDepth, float4 EncodedVelocity)
{
	float DeviceZ = ConvertToDeviceZ(SceneDepth);
	float4 ThisClip = float4(ScreenPos, DeviceZ, 1);
	float4 PrevClip = mul(ThisClip, View.ClipToPrevClip);
	float2 PrevScreenPos = PrevClip.xy / PrevClip.w;
	if (EncodedVelocity.x > 0.0)
	{
		// #yuriy_todo: use full 3D velocity when it's available
		PrevScreenPos = ThisClip.xy - DecodeVelocityFromTexture(EncodedVelocity).xy;
	}

	PrevScreenPos.xy = clamp(PrevScreenPos.xy, (float2)-1, (float2)1);

	return PrevScreenPos.xy * HistoryScreenPositionScaleBias.xy + HistoryScreenPositionScaleBias.zw;
}

// [numthreads(8, 8, 1)]
// void TemporalResamplingCS(uint2 DispatchThreadID : SV_DispatchThreadID) 
// {
//     uint2 DispatchThreadId = DispatchThreadID.xy + View.ViewRectMin.xy;
// 	uint2 PixelCoord = GetPixelCoord(DispatchThreadId, UpscaleFactor);
// 	if (any(PixelCoord >= View.ViewSizeAndInvSize.xy))
// 	{
// 		return;
// 	}

// 	const float2 BufferUV = (PixelCoord + 0.5) * View.BufferSizeAndInvSize.zw;
//     const float2 ViewportUV = BufferUVToViewportUV(BufferUV);
// 	const float2 ScreenPos = ViewportUVToScreenPos(ViewportUV);

// 	FGBufferData GBuffer = GetGBufferDataFromSceneTextures(BufferUV);
// 	float DeviceZ = SceneDepthTexture.Load(int3(PixelCoord, 0)).r;
//     float SceneDepth = ConvertFromDeviceZ(DeviceZ);
    
//     float3 TranslatedWorldPosition = ReconstructTranslatedWorldPositionFromDeviceZ(PixelCoord, DeviceZ);
//     RTXGI_Reservoir state = RTXGI_Reservoir::Empty();

// 	float3 DiffuseColor = GBuffer.DiffuseColor;
// 	const bool bIsDepthValid = DeviceZ > 0.0 && DeviceZ < 0.999;
// 	const bool bIsValidPixel = GBuffer.ShadingModelID != SHADINGMODELID_UNLIT && bIsDepthValid;

// 	float3 WorldNormal = GBuffer.WorldNormal;
//     const float InvFraction = 1.0 / UpscaleFactor;

//     uint LinearIndex = CalcLinearIndex(PixelCoord);
// 	FRandomContext RandContext = FRandomContext::Create(LinearIndex, View.StateFrameIndex + HistoryReservoir * 32);

// 	if (bIsValidPixel)
// 	{
        
// 		RTXGI_Reservoir curSample = RTXGI_Reservoir::Load(ReadReservoirData(DispatchThreadId, InputSlice));

// 		// int historyLimit = min(RTXGI_Reservoir::MaxM,MaxTemporalHistory * curSample.M);

// 		state.CombineReservoirs(curSample, /* random = */ 0.5, curSample.targetPdf);

//         // const float4 EncodedVelocity = Texture2DSampleLevel(GBufferVelocityTexture, GlobalPointClampedSampler, BufferUV, 0);
//         // const float2 PrevUV = GetReprojectedBufferUV(ScreenPos, SceneDepth, EncodedVelocity);
//         // const float2 UVDiff =  PrevUV - BufferUV;
//         		// start by just using our sample position
// 		int2 prevPos = PixelCoord;
// 		float ExpectedPrevLinearDepth = GBuffer.Depth;

// 		float2 ViewUV = (float2(PixelCoord) + 0.5) * View.ViewSizeAndInvSize.zw;
// 		float4 NDC = float4(ViewUV * float2(2, -2) + float2(-1, 1), DeviceZ, 1);

// #if GBUFFER_HAS_VELOCITY && 0
// 		// Some objects can get marked as not having velocities, which leads to DecodeGBuffer zeroing them
// 		// This appears to be errant under some conditions, so overriding the rejection produces better results
// 		GBuffer.Velocity = GBufferVelocityTexture.Load(int3(PixelCoord, 0));
// #endif
// 		if (GBuffer.Velocity.x > 0.0)
// 		{
// 			float2 Velocity = DecodeVelocityFromTexture(GBuffer.Velocity).xy;
// 			float2 PrevNDC = NDC.xy - Velocity;
// 			float2 PrevUV = (PrevNDC.xy * float2(1, -1) + 1) * 0.5;
// 			prevPos = PrevUV * View.ViewSizeAndInvSize.xy + View.ViewRectMin.xy;
// 			ExpectedPrevLinearDepth = ConvertFromDeviceZ(DeviceZ - DecodeVelocityFromTexture(GBuffer.Velocity).z);
// 		}
// 		else
// 		{
// 			float4 PrevNDC = mul(NDC, View.ClipToPrevClip);
// 			PrevNDC.xyz /= PrevNDC.w;
// 			float2 PrevUV = (PrevNDC.xy * float2(1, -1) + 1) * 0.5;
// 			prevPos = PrevUV * View.ViewSizeAndInvSize.xy + View.ViewRectMin.xy;

// 			ExpectedPrevLinearDepth = ConvertFromDeviceZ(PrevNDC.z);
// 		}

        
// 		bool foundNeighbor = false;
// 		const float radius = 4;
// 		// Try to find a matching surface in the neighborhood of the reprojected pixel
// 		for (int i = 0; i < 9; i++)
// 		{
// 			int2 offset = 0;
// 			if (i > 0)
// 			{
// 				offset = int2((RandContext.GenerateSample2D() - 0.5f) * radius);
// 			}
// 			int2 idx = prevPos + offset;

// 			float PrevDepth = ConvertFromDeviceZ(DepthHistory.Load(int3(idx, 0)).r);
// 			float3 PrevWorldNormal = normalize(DecodeNormal(NormalHistory.Load(int3(idx, 0)).xyz));

// 			// TODO: refine sample rejection tests
// 			if (dot(GBuffer.WorldNormal, PrevWorldNormal) < TemporalNormalRejectionThreshold)
// 			{
// 				continue;
// 			}

// 			if (abs(SceneDepth - PrevDepth) / SceneDepth > TemporalDepthRejectionThreshold)
// 			{
// 				continue;
// 			}

// 			prevPos = idx;
// 			foundNeighbor = true;
// 			break;
// 	    }

//         const int SubPixelCount = (UpscaleFactor * UpscaleFactor - 1);
//         const float AngleOffset = ((View.StateFrameIndex + 7) * 11) % 32 * 2 * PI;
//         const uint SpatialSamples = 5;
//        // for (uint SampleIndex = 1; SampleIndex < SpatialSamples && state.M < MaxTemporalHistory * 1.25; ++SampleIndex) 
//         if (foundNeighbor)
//         {
//             // const float Angle = (SampleIndex + AngleOffset) * GOLDEN_ANGLE;
//             // const float PixelOffsetRadius = sqrt(float(((SampleIndex - 1) + View.StateFrameIndex) & SubPixelCount) + 1) * 
//             //                                 clamp(MaxTemporalHistory - state.M, 1, MaxTemporalHistory); // TODO: keep high in noisy situations
            
//             // const float2 PixelOffsetBase = float2(cos(Angle), sin(Angle)) * PixelOffsetRadius;

//             // const int2 PixelOffset = int2(PixelOffsetBase);
//             // int2 ReprojPx = floor(((DispatchThreadId + PixelOffset ^ SubPixelCount) + UVDiff * View.BufferSizeAndInvSize.xy * InvFraction + 0.5));

//             //const int2 ReservoirPixel = clamp(ReprojPx + PixelOffset, uint2(0,0), ReservoirHistoryBufferDim.xy - 1 );
//             const int2 ReservoirPixel = clamp(floor(InvFraction * prevPos), uint2(0,0), ReservoirHistoryBufferDim.xy - 1 );
//             const uint2 HighReservoirPixel = GetPixelCoord(ReservoirPixel, UpscaleFactor);

//             float PrevDepth = ConvertFromDeviceZ(DepthHistory.Load(int3(HighReservoirPixel, 0)).r);
// 			float3 PrevWorldNormal = normalize(DecodeNormal(NormalHistory.Load(int3(HighReservoirPixel, 0)).xyz));

// 			// TODO: refine sample rejection tests
// 			// if (dot(GBuffer.WorldNormal, PrevWorldNormal) < TemporalNormalRejectionThreshold)
// 			// {
// 			// 	continue;
// 			// }

// 			// if (abs(SceneDepth - PrevDepth) / SceneDepth > TemporalDepthRejectionThreshold)
// 			// {
// 			// 	continue;
// 			// }

//             RTXGI_Reservoir prevSample = RTXGI_Reservoir::Load(ReadReservoirHistoryData(ReservoirPixel, HistoryReservoir));
//             prevSample.M = min(prevSample.M, MaxTemporalHistory);
//             if (prevSample.sampleRef.IsValid())
//             {
//                 float Jocbian = 1.0f;
//                 float pNewTN = GetTargetPdf(prevSample.sampleRef.Irradiance, WorldNormal, TranslatedWorldPosition, prevSample.sampleRef.Position);
//                 float pTargetPdf = pNewTN * Jocbian ;
//                 state.CombineReservoirs(prevSample, RandContext.GenerateSample1D(), pTargetPdf);
//             }
//         }
//         state.FinalizeResampling(1, state.M);
//     }
//     WriteReservoirData(DispatchThreadId, OutputSlice, state.Store());
// }


/***************************************************************************************************
 *
 *  ApplyTemporalResampling
 ***************************************************************************************************/
RAY_TRACING_ENTRY_RAYGEN(ApplyTemporalResamplingRGS)
{
	uint2 DispatchThreadId = DispatchRaysIndex().xy + View.ViewRectMin.xy;
	uint2 DispatchDimension = DispatchRaysDimensions().xy;
	uint2 PixelCoord = GetPixelCoord(DispatchThreadId, UpscaleFactor);
	if (any(PixelCoord > View.ViewSizeAndInvSize.xy))
	{
		return;
	}

	uint LinearIndex = CalcLinearIndex(PixelCoord);
	FRandomContext RandContext = FRandomContext::Create(LinearIndex, View.StateFrameIndex + HistoryReservoir * 32);

	FGBufferData GBuffer = GetGBufferDataFromSceneTexturesLoad(PixelCoord);
	float DeviceZ = SceneDepthTexture.Load(int3(PixelCoord, 0)).r;
	float3 WorldPosition;
	float3 CameraDirection;
	ReconstructTranslatedWorldPositionAndCameraDirectionFromDeviceZ(PixelCoord, DeviceZ, WorldPosition, CameraDirection);

	RTXGI_Reservoir state = RTXGI_Reservoir::Empty();

	float3 DiffuseColor = GBuffer.DiffuseColor;
	const bool bIsDepthValid = DeviceZ > 0.0;
	const bool bIsValidPixel = GBuffer.ShadingModelID != SHADINGMODELID_UNLIT && bIsDepthValid && Luminance(DiffuseColor) > DiffuseThreshold;

	float3 WorldNormal = GBuffer.WorldNormal;
	if (bIsValidPixel)
	{
		RTXGI_Reservoir curSample = RTXGI_Reservoir::Load(ReadReservoirData(DispatchThreadId, InputSlice));

		int historyLimit = min(RTXGI_Reservoir::MaxM,MaxTemporalHistory * curSample.M);

		state.CombineReservoirs(curSample, /* random = */ 0.5, curSample.targetPdf);

		// Backproject this pixel to last frame

		// start by just using our sample position
		int2 prevPos = PixelCoord;
		float ExpectedPrevLinearDepth = GBuffer.Depth;

		float2 ViewUV = (float2(PixelCoord) + 0.5) * View.ViewSizeAndInvSize.zw;
		float4 NDC = float4(ViewUV * float2(2, -2) + float2(-1, 1), DeviceZ, 1);

#if GBUFFER_HAS_VELOCITY && 0
		// Some objects can get marked as not having velocities, which leads to DecodeGBuffer zeroing them
		// This appears to be errant under some conditions, so overriding the rejection produces better results
		GBuffer.Velocity = GBufferVelocityTexture.Load(int3(PixelCoord, 0));
#endif
		if (GBuffer.Velocity.x > 0.0)
		{
			float2 Velocity = DecodeVelocityFromTexture(GBuffer.Velocity).xy;
			float2 PrevNDC = NDC.xy - Velocity;
			float2 PrevUV = (PrevNDC.xy * float2(1, -1) + 1) * 0.5;
			prevPos = PrevUV * View.ViewSizeAndInvSize.xy + View.ViewRectMin.xy;
			ExpectedPrevLinearDepth = ConvertFromDeviceZ(DeviceZ - DecodeVelocityFromTexture(GBuffer.Velocity).z);
		}
		else
		{
			float4 PrevNDC = mul(NDC, View.ClipToPrevClip);
			PrevNDC.xyz /= PrevNDC.w;
			float2 PrevUV = (PrevNDC.xy * float2(1, -1) + 1) * 0.5;
			prevPos = PrevUV * View.ViewSizeAndInvSize.xy + View.ViewRectMin.xy;

			ExpectedPrevLinearDepth = ConvertFromDeviceZ(PrevNDC.z);
		}


		//ToDo - full GBuffer not available for last frame, so we're going to need to approximate using current + channels we have
		// could make a better approximation than what is used here
		FGBufferData PrevGBuffer = GBuffer;
		bool foundNeighbor = false;
		const float radius = 4;

		// Try to find a matching surface in the neighborhood of the reprojected pixel
		for (int i = 0; i < 9; i++)
		{
			int2 offset = 0;
			if (i > 0)
			{
				offset = int2((RandContext.GenerateSample2D() - 0.5f) * radius);
			}
			int2 idx = prevPos + offset;

			float PrevDepth = ConvertFromDeviceZ(DepthHistory.Load(int3(idx, 0)).r);
			float3 PrevWorldNormal = normalize(DecodeNormal(NormalHistory.Load(int3(idx, 0)).xyz));

			// TODO: refine sample rejection tests
			if (dot(GBuffer.WorldNormal, PrevWorldNormal) < TemporalNormalRejectionThreshold)
			{
				continue;
			}

			if (abs(ExpectedPrevLinearDepth - PrevDepth) / ExpectedPrevLinearDepth > TemporalDepthRejectionThreshold)
			{
				continue;
			}

			PrevGBuffer.WorldNormal = PrevWorldNormal;
			PrevGBuffer.Depth = PrevDepth;

			prevPos = idx;
			foundNeighbor = true;
			break;
		}

		bool selectedPreviousSample = false;
		uint previousM = 0;
		float previousWeight = 0;

		if (foundNeighbor)
		{
			// Resample the previous frame sample into the current reservoir, but reduce the gisample's weight
			// according to the bilinear weight of the current pixel
			const uint2 prePosId = clamp(uint2(prevPos / float(UpscaleFactor)), uint2(0,0), ReservoirBufferDim.xy - 1);

			RTXGI_Reservoir prevSample = RTXGI_Reservoir::Load(ReadReservoirHistoryData(prePosId, HistoryReservoir));
			prevSample.M = min(prevSample.M, historyLimit);

			if (prevSample.sampleRef.IsValid())
			{
				bool Visible = true;
#if (VISIBILITY_BEFORE_COMBINE)
				if (ApplyApproximateVisibilityTest)
				{
					Visible = CheckApproximateVisibility(PixelCoord, DeviceZ, WorldPosition, GBuffer, prevSample.sampleRef);
				}
#endif
				//previousWeight = Visible ? GetApproximateLightSampleWeight(WorldPosition, CameraDirection, GBuffer, prevSample.sampleRef) : 0;
				float Jocbian = 1.0f;
				float pNewTN = GetTargetPdf(prevSample.sampleRef.Irradiance, WorldNormal, WorldPosition, prevSample.sampleRef.Position);
				previousWeight = Visible ?  pNewTN * Jocbian : 0;
				{
					previousM = prevSample.M;

					if( state.CombineReservoirs(prevSample, RandContext.GenerateSample1D(), previousWeight) )
					{
						selectedPreviousSample = true;
					}
				}
			}
		}

#if (!TEMPORAL_RESTIR_BIAS)		
		// Compute the unbiased normalization term (instead of using 1/M)
		float pi = state.targetPdf;   // Since it was selected, this is known to be equiv to lightWeight(state.sampleRef, context)
		float piSum = state.targetPdf * curSample.M;

		if (state.sampleRef.IsValid() && previousM > 0)
		{
			float pt = GetApproximateLightSampleWeight(WorldPosition, CameraDirection, GBuffer, state.sampleRef);

#if (!VISIBILITY_BEFORE_COMBINE)
			//ToDo - does this need to be a permutation?
			if (ApplyApproximateVisibilityTest && pt > 0)
			{
				bool Visible = CheckApproximateVisibility(PixelCoord, DeviceZ, WorldPosition, GBuffer, state.sampleRef);

				if (!Visible)
				{
					pt = 0;
				}
			}
#endif

			pi = selectedPreviousSample ? pt : pi;
			piSum += pt * previousM;
		}

		state.FinalizeResampling(pi, piSum);
#else
		// If the prior reservoir actually corresponds to the current reservoir, then the visibility and weights should all be the same, and cancel out.
		state.FinalizeResampling(1, state.M);
#endif
	}

	WriteReservoirData(DispatchThreadId, OutputSlice, state.Store());
}
